//  Copyright 2013 Google Inc. All Rights Reserved.
//
//  Licensed under the Apache License, Version 2.0 (the "License");
//  you may not use this file except in compliance with the License.
//  You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
//  Unless required by applicable law or agreed to in writing, software
//  distributed under the License is distributed on an "AS IS" BASIS,
//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//  See the License for the specific language governing permissions and
//  limitations under the License.

#include <map>
#include <vector>
#include <algorithm>
#include "PerfectMatching.h"

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <math.h>
#include <pthread.h>

using namespace std;

#define MAX_BLOCK 1024
#define MAX_STRING 1024
#define EXP_TABLE_SIZE 1000
#define MAX_EXP 6
#define MAX_SENTENCE_LENGTH 1000
#define MAX_CODE_LENGTH 40

const int vocab_hash_size = 30000000;	// Maximum 30 * 0.7 = 21M words in the vocabulary
const double NormalBase = -1000000.0;
const int top_k = 30; // for every word in similarity matrix, maintain its top_k most similar neighbors
typedef double real;			// Precision of float numbers

FILE *fgraph;				// 调试时用于输出图
map<pair<int,int>, int> Edge;       //Li Caihua
map<pair<int,int>, double> Edge2;   //Ruan Chong
bool DebugGraph = true;		// 是否需要输出调试信息
bool DebugHuffman = true; 	// Output Huffman tree before and after ajustment.


//把同一层的Huffman树节点组织成一个链表。链表的表头和节点类型定义。
struct eType
{
    int r, next;
}*e, *e2, *Floor, Floor_head[MAX_CODE_LENGTH];
/// e[i].r: equivalent to e[i].to, i.e. there is an edge (i, e[i].r) in Huffman DAG.
/// e[i].next: index of next out-edge of the current node.

/// e2[i].r: id in word list for current word e2[i].
/// e2[i].next: next word in the same subtree.

/// Floor[i].r: unused.
/// Floor[i].next: next node in the linked list of layer i.

/// Floor_head[i].next: head of linked list of layer i.
/// Floor_head[i].r: number of nodes in the linked list of layer i.



/// Modified by RuanChong
struct hs_parameter { // Softmax Layer parameters
    char *code;
    int codelen;
};

hs_parameter* hs_para;
// Modification ends.


/// cn是词频计数(counter)，word数组是单词本身，code数组是单词的Huffman编码
/// codelen是Huffman编码的长度，point数组是单词的祖先的列表（从根节点一直到它自身）
struct vocab_word {
    long long cn;
    int *point;
    char *word, *code, codelen;
};

const char* ontology_file, *sim_file, *wordlist_file;
char train_file[MAX_STRING], output_file[MAX_STRING];
char save_vocab_file[MAX_STRING], read_vocab_file[MAX_STRING];
struct vocab_word *vocab;
int line, *head, tree_size, root, *Dep, maxDep, waist, block_max_size = 1000, block_size = 0, **block;
int binary_sign = 0, cbow = 1, debug_mode = 2, window = 5, min_count = 5, num_threads = 12, min_reduce = 1, testing = 0;
int *vocab_hash;
long long vocab_max_size = 1000, vocab_size = 0, layer1_size = 100;
long long train_words = 0, word_count_actual = 0, iter = 5, file_size = 0, classes = 0;
long long *parent_node, *belong, *binary;
real alpha = 0.025, starting_alpha, sample = 1e-3; // sample: subsampling for frequent words.
real *syn0, *syn1, *syn1neg, *expTable; // syn0用于存储词向量, syn1用于存储Huffman树内部节点的参数, syn1neg用于负采样, expTable用于存储sigmoid函数值，通过查表来减少计算量
clock_t start;


int settle_down_compiler_int = 0; // Used to accept return values of fscanf(), fgets(), etc.
char* settle_down_compiler_pointer = NULL;

real *sim_mat;
int *ind_mat;
long long word_list_vocab_size = 0;
int* index_in_word_list;    // Brute force mapping.  Some memory may be wasted.
// index_in_word_list[id in word2vec] = id in word list file generated by Xiaoliu.

int hs = 0, negative = 5;
const int table_size = 1e8;
int *table;


void SaveVocabDebug(char* file_name) {
    long long i;
    FILE *fo = fopen(file_name, "wb");
    for (i = 0; i < vocab_size; i++) fprintf(fo, "%s %lld\n", vocab[i].word, vocab[i].cn);
    fclose(fo);
}

/// Modified by RuanChong
///给每个顶点维护一个前 top_k 最相似的词的列表
inline void sim_mat_insert(int a, int b, real sim)
{
    for (int ii=0; ii<top_k; ii++)
        if (sim > sim_mat[a * top_k + ii])
        {
            for (int jj=top_k-2; jj>=ii; jj--)
            {
                sim_mat[a * top_k + jj] = sim_mat[a * top_k + jj+1];
                ind_mat[a * top_k + jj] = ind_mat[a * top_k + jj+1];
            }
            sim_mat[a * top_k + ii] = sim;
            ind_mat[a * top_k + ii] = b;
            break;
        }
}

/// Modified by RuanChong
inline real sim_mat_value(int a, int b)
{
    for (int ii=0; ii<top_k; ii++)
        if (b == ind_mat[a * top_k + ii])
        {
            return sim_mat[a * top_k + ii];
        }
    return 0.0;
}

// 负采样用，以 vocab[].cn**0.75 为权重
void InitUnigramTable() {
    int a, i;
    double train_words_pow = 0;
    double d1, power = 0.75;
    table = (int *)malloc(table_size * sizeof(int));
    for (a = 0; a < vocab_size; a++) train_words_pow += pow(vocab[a].cn, power);
    i = 0;
    d1 = pow(vocab[i].cn, power) / train_words_pow;
    for (a = 0; a < table_size; a++) {
        table[a] = i;
        if (a / (double)table_size > d1) {
            i++;
            d1 += pow(vocab[i].cn, power) / train_words_pow;
        }
        if (i >= vocab_size) i = vocab_size - 1;
    }
}

// Reads a single word from a file, assuming space + tab + EOL to be word boundaries
void ReadWord(char *word, FILE *fin) {
    int a = 0, ch;
    while (!feof(fin)) {
        ch = fgetc(fin);
        if (ch == 13) continue;
        if ((ch == ' ') || (ch == '\t') || (ch == '\n')) {
            if (a > 0) {
                if (ch == '\n') ungetc(ch, fin);
                break;
            }
            if (ch == '\n') {
                strcpy(word, (char *)"</s>");
                return;
            } else continue;
        }
        word[a] = ch;
        a++;
        if (a >= MAX_STRING - 1) a--;   // Truncate too long words
    }
    word[a] = 0;
}

// Returns hash value of a word
int GetWordHash(char *word) {
    unsigned long long a, hash = 0;
    for (a = 0; a < strlen(word); a++) hash = hash * 257 + word[a];
    hash = hash % vocab_hash_size;
    return hash;
}

// Returns position of a word in the vocabulary; if the word is not found, returns -1
int SearchVocab(char *word) {
    unsigned int hash = GetWordHash(word);
    while (1) {
        if (vocab_hash[hash] == -1) return -1;
        if (!strcmp(word, vocab[vocab_hash[hash]].word)) return vocab_hash[hash];
        hash = (hash + 1) % vocab_hash_size;
    }
    return -1;
}

// Reads a word and returns its index in the vocabulary
int ReadWordIndex(FILE *fin) {
    char word[MAX_STRING];
    ReadWord(word, fin);
    if (feof(fin)) return -1;
    return SearchVocab(word);
}

// Adds a word to the vocabulary
int AddWordToVocab(char *word) {
    unsigned int hash, length = strlen(word) + 1;
    if (length > MAX_STRING) length = MAX_STRING;
    vocab[vocab_size].word = (char *)calloc(length, sizeof(char));
    strcpy(vocab[vocab_size].word, word);
    vocab[vocab_size].cn = 0;
    vocab_size++;
    // Reallocate memory if needed
    if (vocab_size + 2 >= vocab_max_size) {
        vocab_max_size += 1000;
        vocab = (struct vocab_word *)realloc(vocab, vocab_max_size * sizeof(struct vocab_word));
    }
    hash = GetWordHash(word);
    while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;
    vocab_hash[hash] = vocab_size - 1;
    return vocab_size - 1;
}

// Used later for sorting by word counts
int VocabCompare(const void *a, const void *b) {
    return ((struct vocab_word *)b)->cn - ((struct vocab_word *)a)->cn;
}

// Sorts the vocabulary by frequency using word counts
void SortVocab() {
    int a, size;
    unsigned int hash;
    // Sort the vocabulary and keep </s> at the first position
    qsort(&vocab[1], vocab_size - 1, sizeof(struct vocab_word), VocabCompare);
    for (a = 0; a < vocab_hash_size; a++) vocab_hash[a] = -1;
    size = vocab_size;
    train_words = 0;
    for (a = 0; a < size; a++) {
        // Words occuring less than min_count times will be discarded from the vocab
        if ((vocab[a].cn < min_count) && (a != 0)) {
            vocab_size--;
            free(vocab[a].word);
        } else {
            // Hash will be re-computed, as after the sorting it is not actual
            hash=GetWordHash(vocab[a].word);
            while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;
            vocab_hash[hash] = a;
            train_words += vocab[a].cn;
        }
    }
    vocab = (struct vocab_word *)realloc(vocab, (vocab_size + 1) * sizeof(struct vocab_word));
    // Allocate memory for the binary tree construction
    for (a = 0; a < vocab_size; a++) {
        vocab[a].code = (char *)calloc(MAX_CODE_LENGTH, sizeof(char));
        vocab[a].point = (int *)calloc(MAX_CODE_LENGTH, sizeof(int));
    }
}

// Reduces the vocabulary by removing infrequent tokens
void ReduceVocab() {
    int a, b = 0;
    unsigned int hash;
    for (a = 0; a < vocab_size; a++) if (vocab[a].cn > min_reduce) {
        vocab[b].cn = vocab[a].cn;
        vocab[b].word = vocab[a].word;
        b++;
    } else free(vocab[a].word);
    vocab_size = b;
    for (a = 0; a < vocab_hash_size; a++) vocab_hash[a] = -1;
    for (a = 0; a < vocab_size; a++) {
        // Hash will be re-computed, as it is not actual
        hash = GetWordHash(vocab[a].word);
        while (vocab_hash[hash] != -1) hash = (hash + 1) % vocab_hash_size;
        vocab_hash[hash] = a;
    }
    fflush(stdout);
    min_reduce++;
}

void addEdge(int x, int y) {
    e[line].r = y;
    e[line].next = head[x];
    head[x] = line++;
}


/// e[i].r: equivalent to e[i].to, i.e. there is an edge (i, e[i].r) in Huffman DAG.
/// e[i].next: index of next out-edge of the current node.

/// Floor[i].r: unused.
/// Floor[i].next: next node in the linked list of layer i.

/// 每一层的表头，保存相应链表的第一个节点 和 链表的大小（即该层节点数） 这两项信息
/// Floor_head[i].next: head of linked list of layer i.
/// Floor_head[i].r: number of nodes in the linked list of layer i.

// 对以节点x为根的子树进行DFS，同时把子树里所有的节点 按层 串成链表。节点x的深度为dep（根节点深度为1）
void DfsBinaryTree(int x, int dep) {
    Dep[x] = dep; // Dep[x]为节点x的深度
    Floor[x].next = Floor_head[dep].next;
    Floor_head[dep].next = x;
    ++Floor_head[dep].r;
    int i;
    for (i = head[x]; i != -1; i = e[i].next) // head[i]: head[] for Huffman DAG.
        DfsBinaryTree(e[i].r, dep+1);
}

//用并查集查询x所在的子树的根节点
int get(int x) {
    if(belong[x] == x) return x;
    return belong[x] = get(belong[x]);
}

// 原图G = (V, E)，其中|V| = tot，E = Edge，建用于匹配的图G' = (V', E'),
// 其中V' = V ∪ V~，E' = E ∪ E~ ∪ {uu~}，并返回其点数node_num和边数edge_num
// V~是V的副本，E~是E的副本，从而把 最大权匹配 规约成 最大权完美匹配
// 此外，还有几个细节要注意：
// 1，先前未将边权归一化，此函数中需进行归一化处理
// 2,乘一个NormalBase（一个很大的负数）再取整，把最大权完美匹配 转化成 最小权完美匹配，同时避免浮点运算
PerfectMatching* BuildGraph(int tot, map<pair<int,int>,int> &Edge, long long *subtree_size, int *old_id, int &node_num, int &edge_num) {
    node_num = tot*2; edge_num = 2*Edge.size()+tot;
    PerfectMatching *pm = new PerfectMatching(node_num, edge_num);
    map<pair<int,int>,int>::iterator ptrE;
    if (DebugGraph)
        fprintf(fgraph, "%d %d\n", node_num, edge_num);
    // 统计同义词对的分布情况，用于Debug
    // C[k]的含义是，相似性为k（归一化前的值，也即同义词对的数目）的节点对（也即子树对）的个数
    map<int,int> C;
    C.clear();
    for (ptrE = Edge.begin(); ptrE != Edge.end(); ++ptrE) {
        int i = ptrE->first.first;
        int j = ptrE->first.second;
        long long tmp = subtree_size[old_id[i]] * subtree_size[old_id[j]]; // 归一化
        tmp = (int)(NormalBase*ptrE->second/tmp);
        pm->AddEdge(i, j, tmp);			// E
        pm->AddEdge(i+tot, j+tot, tmp);		// E~
        if (DebugGraph)
        {
            fprintf(fgraph, "%d %d %lld\n", i, j, tmp);
            fprintf(fgraph, "%d %d %lld\n", i+tot, j+tot, tmp);
        }
        C[ptrE->second]++;
    }
    for (int i = 0; i < tot; ++i) {
        pm->AddEdge(i, tot+i, 0);			// {uu~}
        if (DebugGraph) {
            fprintf(fgraph, "%d %d %d\n", i, tot+i, 0);
        }
    }
    if (DebugGraph) {
        fprintf(fgraph, "total\n");
        for (map<int,int>::iterator it = C.begin(); it != C.end(); ++it) {
            fprintf(fgraph, "%d %d\n", it->first, it->second);
        }
    }
    return pm;
}



// 原图G = (V, E)，其中|V| = tot，E = Edge，建用于匹配的图G' = (V', E'),
// 其中V' = V ∪ V~，E' = E ∪ E~ ∪ {uu~}，并返回其点数node_num和边数edge_num
// V~是V的副本，E~是E的副本，从而把 最大权匹配 规约成 最大权完美匹配
// 此外，还有几个细节要注意：
// 1，先前未将边权归一化，此函数中需进行归一化处理
// 2,乘一个NormalBase（一个很大的负数）再取整，把最大权完美匹配 转化成 最小权完美匹配，同时避免浮点运算
PerfectMatching* BuildGraph2(int tot, map<pair<int,int>, double> &Edge, long long *subtree_size, int *old_id, int &node_num, int &edge_num) {
    clock_t build_graph_time1 = clock();
    node_num = tot*2; edge_num = 2*Edge.size()+tot;
    PerfectMatching *pm = new PerfectMatching(node_num, edge_num);
    map<pair<int,int>, double>::iterator ptrE;
    if (DebugGraph)
        fprintf(fgraph, "%d %d\n", node_num, edge_num);
    // map<int,int> C;                 // 统计在同一个团内的数对个数，用于Debug
    // C.clear();
    for (ptrE = Edge.begin(); ptrE != Edge.end(); ++ptrE) {
        int i = ptrE->first.first;
        int j = ptrE->first.second;
        long long tmp = subtree_size[old_id[i]] * subtree_size[old_id[j]];
        tmp = (int)(NormalBase * ptrE->second / tmp);
        pm->AddEdge(i, j, tmp);         // E
        pm->AddEdge(i+tot, j+tot, tmp);     // E~
        if (DebugGraph)
        {
            fprintf(fgraph, "%d %d %lld\n", i, j, tmp);
            fprintf(fgraph, "%d %d %lld\n", i+tot, j+tot, tmp);
        }
        // C[ptrE->second]++;
    }
    for (int i = 0; i < tot; ++i) {
        pm->AddEdge(i, tot+i, 0);           // {uu~}
        if (DebugGraph) {
            fprintf(fgraph, "%d %d %d\n", i, tot+i, 0);
        }
    }
    // if (DebugGraph) {
    //     fprintf(fgraph, "total\n");
    //     for (map<int,int>::iterator it = C.begin(); it != C.end(); ++it) {
    //         fprintf(fgraph, "%d %d\n", it->first, it->second);
    //     }
    // }
    clock_t build_graph_time2 = clock();
    printf("build_graph_time: %.4f\n", (double)(build_graph_time2 - build_graph_time1)/CLOCKS_PER_SEC);
    return pm;
}


// 根据本体中的同义词集 调整 Huffman 树结构
void AdjustBinaryTree() {
    printf("Adjusting Huffman tree according to Ontology\n");

    bool flag;
    long long tmp;
    int node_num, edge_num;
    int i, j, k, l, r, x, y, dep, tot, size;
    tree_size = vocab_size*2-1;					// 根据parent_node构树, tree_size include both lead nodes and internal nodes.
    root = tree_size - 1;
    line = 0;
    head = (int *)calloc(tree_size+1, sizeof(int));
    for (i = 0; i <= tree_size; ++i) head[i] = -1;
    e = (eType*)calloc(tree_size+1, sizeof(eType));
    for (i = 0; i < tree_size-1; ++i) // Construct a DAG according to Huffman Tree
        addEdge(parent_node[i], i);
    Dep = (int*)calloc(tree_size+1, sizeof(int));		// 把Huffman分层
    Floor = (eType*)calloc(tree_size+1, sizeof(eType));
    for (i = 0; i < MAX_CODE_LENGTH; ++i) {
        Floor_head[i].next = -1;
        Floor_head[i].r = 0;
    }
    DfsBinaryTree(root, 1);
    for (maxDep = 0, waist = 0; Floor_head[maxDep+1].r;)
        if (Floor_head[++maxDep].r > waist) waist = Floor_head[maxDep].r;
    printf("Finished collecting infomation of Huffman tree\n");
    printf("maxDep : %d\twaist : %d\n", maxDep, waist);
    free(head);
    free(e);

    int *lson = (int*)calloc(tree_size+1, sizeof(int));
    int *rson = (int*)calloc(tree_size+1, sizeof(int));
    int *new_id = (int*)calloc(tree_size+1, sizeof(int));
    int *old_id = (int*)calloc(tree_size+1, sizeof(int));
    belong = (long long*)calloc(tree_size+1, sizeof(long long));
    long long* subtree_size = (long long*)calloc(tree_size + 1, sizeof(long long));

    for(int i = 0; i <= tree_size; i++) {
        belong[i] = i;
    }

    for (dep = maxDep; dep > 1; --dep)			// 从下往上逐层调整
    {
        tot = 0;					// 更新Huffman第dep层信息，以便后续调整该层节点
        for (x = Floor_head[dep].next; x != -1; x = Floor[x].next)
        {
            new_id[x] = tot; old_id[tot++] = x; //给当前层的节点分配新id，并维护其与旧id的对应关系
            if (x < vocab_size) {
                subtree_size[x] = 1;
            } else {
                l = lson[x]; r = rson[x];
                subtree_size[x] = subtree_size[l] + subtree_size[r];
                belong[l] = belong[r] = x;
            }
        }
        printf("adjusting level %d with %d nodes\n", dep, tot);
        Edge.clear();
        map<pair<int,int>,int>::iterator ptrE;
        for (i = 0; i < block_size; ++i) //通过遍历所有的block来计算子树两两之间的相似性。每个block是一组同义词，block[i][0]为同义词集的大小，其余元素为该block里的单词。
        {
            size = block[i][0];
            for (j = 1; j < size; ++j)
                for (k = j+1; k <= size; ++k)
                {
                    if(Dep[block[i][j]] < dep || Dep[block[i][k]] < dep) {
                        continue;
                    }
                    x = new_id[get(block[i][j])];
                    y = new_id[get(block[i][k])];
                    if (x == y) continue;
                    if (x > y) {tmp = x; x = y; y = tmp;}
                    ptrE = Edge.find(make_pair(x, y));			// 加边
                    if (ptrE == Edge.end()) //修改以x和y为根节点的子树的同义词数目
                        Edge.insert(make_pair(make_pair(x,y), 1));
                    else
                        ++(ptrE->second);
                }
        }
	// 将构建的相似性图写入文件中以便检查
        char graph_file_name[20];
        sprintf(graph_file_name, "graph%02d.txt", dep);
        printf("depth = %d, graph_file_name = %s\n", dep, graph_file_name);
        if (DebugGraph)
        	fgraph = fopen(graph_file_name, "w");
        PerfectMatching *pm = BuildGraph(tot, Edge, subtree_size, old_id, node_num, edge_num);
        if (DebugGraph)
        	fclose(fgraph);
        pm->Solve();							// 调用模板进行最大权匹配
        int numMatchs = 0;
        for (x = Floor_head[dep-1].next; x != -1 && x < vocab_size; x = Floor[x].next)
            ; // find an internal node in the upper layer.
        for (i = 0; i < tot; ++i)					// 把匹配的点对两个两个地连向上一层的同一个父亲
        {
            j = pm->GetMatch(i);
            if (i < j && j < tot)
            {
                numMatchs += 1;
                l = old_id[i];
                r = old_id[j];
                lson[x] = l; rson[x] = r;
                binary[l] = 0; binary[r] = 1; //之前修改的变量都是自己新开辟的，这里才更改了word2vec中本来就有的变量
                parent_node[l] = parent_node[r] = x;
                if (x == -1) printf("Error at line %d: %d %d %d\n", __LINE__, dep, l, r);
                for (x = Floor[x].next; x != -1 && x < vocab_size; x = Floor[x].next)
                    ;
            }
        }
        printf("number of matches: %d\n", numMatchs);
        flag = 0;
        for (i = 0; i < tot; ++i)		// 未匹配的点随意连向上一层
        {
            j = pm->GetMatch(i);
            if (j >= tot)
            {
                y = old_id[i];
                if (x == -1) printf("Error at line %d: %d %d\n", __LINE__, dep, y);
                if (flag) {
                    rson[x] = y;
                    binary[y] = 1;
                    parent_node[y] = x;
                    for (x = Floor[x].next; x != -1 && x < vocab_size; x = Floor[x].next);
                } else {
                    lson[x] = y;
                    binary[y] = 0;
                    parent_node[y] = x;
                }
                flag ^= 1;
            }
        }
        delete pm;
        printf("\n");
    }
    printf("Finished adjusting Huffman tree\n");
    free(Floor);
    free(lson);
    free(rson);
    free(new_id);
    free(old_id);
    free(belong);
    free(Dep);
}


// 根据生成本体所用的相似性矩阵 调整 Huffman 树结构
void AdjustBinaryTreeUsingSimMatrix() {
    printf("Begin Adjusting Binary Tree Using Similarity Matrix\n");
    clock_t Adjust_huffman_time1 = clock();
    bool flag;
    long long tmp;
    int node_num, edge_num;
    int i, j, k, l, r, x, y, dep, tot, size;
    tree_size = vocab_size*2-1;                 // 根据parent_node构树
    root = tree_size - 1;
    line = 0;
    head = (int *)calloc(tree_size+1, sizeof(int));
    for (i = 0; i <= tree_size; ++i) head[i] = -1;
    e = (eType*)calloc(tree_size+1, sizeof(eType));
    /* 把用父节点指针保存的Huffman树 转换成 链式前向星存储的DAG，边的方向为父节点指向子节点 */
    for (i = 0; i < tree_size-1; ++i)
        addEdge(parent_node[i], i);
    Dep = (int*)calloc(tree_size+1, sizeof(int));       // 把Huffman分层
    Floor = (eType*)calloc(tree_size+1, sizeof(eType));
    // 每一层的链表初始化为空：next指针设为空，链表长度设为0.
    for (i = 0; i < MAX_CODE_LENGTH; ++i) {
        Floor_head[i].next = -1;
        Floor_head[i].r = 0;
    }
    DfsBinaryTree(root, 1);
    for (maxDep = 0, waist = 0; Floor_head[maxDep+1].r;)
        if (Floor_head[++maxDep].r > waist) waist = Floor_head[maxDep].r;
    printf("Finished collecting infomation of Huffman tree\n");
    printf("maxDep : %d\twaist : %d\n", maxDep, waist);
    free(head);
    free(e);

    int *lson = (int*)calloc(tree_size+1, sizeof(int));
    int *rson = (int*)calloc(tree_size+1, sizeof(int));
    int *new_id = (int*)calloc(tree_size+1, sizeof(int));
    int *old_id = (int*)calloc(tree_size+1, sizeof(int));
    belong = (long long*)calloc(tree_size+1, sizeof(long long));
    long long* subtree_size = (long long*)calloc(tree_size + 1, sizeof(long long));

    //并查集初始化。用并查集来判断是否两棵子树是否已经被连接到了一起
    for(int i = 0; i <= tree_size; i++) {
        belong[i] = i;
    }

    int* subtree_leaf_list_head = (int *)calloc(tree_size+1, sizeof(int));
    memset(subtree_leaf_list_head, -1, (tree_size+1) * sizeof(int));
    e2 = (eType*)calloc(tree_size+1, sizeof(eType));
    if (e2 == NULL)
    {
        printf("Memory error when allocating e2!\n");
        exit(2);
    }

    // 输出最底层原来的匹配，调试用
    int *old_match = NULL;
    FILE* old_match_file = NULL;
    FILE* adjusted_match_file = NULL;
    if (DebugGraph)
    {
        old_match = (int*)calloc(tree_size+1, sizeof(int));
        memset(old_match, -1, (tree_size+1) * sizeof(int));
        old_match_file = fopen("old_match_file.txt", "w");
        adjusted_match_file = fopen("adjusted_match_file.txt", "w");
    }


    int tot2 = 0; // 既在 Huffman树 又在 本体相似性矩阵 里的总词数
    for (dep = maxDep; dep > 1; --dep)          // 从下往上逐层调整
    {
        tot = 0;                    // 更新Huffman第dep层信息，以便后续调整该层节点
        for (x = Floor_head[dep].next; x != -1; x = Floor[x].next)
        {
            // 对于最底层的叶子，打印出它们原来的匹配
            if (DebugGraph && (dep == maxDep))
            {
              if (old_match[parent_node[x]] == -1)
              {
                // 之前未访问过，说明节点 x 是 parent_node[x] 的第一个子节点
                old_match[parent_node[x]] = x;
              }
              else
              {
                // 之前访问过，说明节点 x 和 节点 y = old_match[parent_node[x]] 配对
                y = old_match[parent_node[x]];
                fprintf(old_match_file, "%s %s\n", vocab[x].word, vocab[y].word);
              }
            }

            //给当前层的节点分配新id，并维护其与旧id的对应关系
            //新id从0开始递增，旧id就是 叶节点或内部节点 在构建Huffman树时的count[]数组中的id
            new_id[x] = tot; old_id[tot++] = x;
            if (x < vocab_size) { //若当前节点为叶节点
                subtree_size[x] = 1;
                int tmp = index_in_word_list[x];
                if (tmp > -1)  // if this word is in word list
                {
                    /// 把以x为节点的子树中的（包含在本体里的）单词组织成一个链表
                    /// e2[i].r: id in word list for the current word.
                    /// e2[i].next: next word in the same subtree.
                    // printf("x = %d, index_in_word_list[x] = %d\n", x, tmp);
                    e2[tot2].r = tmp;   // id in word list
                    e2[tot2].next = subtree_leaf_list_head[x]; // next leaf node
                    subtree_leaf_list_head[x] = tot2++;
                }

            } else {
                l = lson[x]; r = rson[x];
                subtree_size[x] = subtree_size[l] + subtree_size[r];
                belong[l] = belong[r] = x;
                // 把左右子树的单词的链表拼起来，作为当前节点包含的单词的链表
                int prev = -1;
                // 寻找最后一个非空节点
                for (int tmp = subtree_leaf_list_head[l]; tmp != -1; tmp = e2[tmp].next)
                    prev = tmp;
                if (prev != -1) //左子树的链表非空
                {
                    e2[prev].next = subtree_leaf_list_head[r];
                    subtree_leaf_list_head[x] = subtree_leaf_list_head[l];
                }
                else
                    subtree_leaf_list_head[x] = subtree_leaf_list_head[r];
            }
        }

        printf("adjusting level %d with %d nodes\n", dep, tot);
        Edge2.clear();
        map<pair<int,int>, double>::iterator ptrE;

        long long total_pairs = (long long) tot * (tot - 1) / 2;
        long long dbg_processed_pairs = 0;
        // 对该层的所有节点之间两两根据相似性连边，构建相似性图 用于最大匹配
        for (int nodei = 0; nodei < tot; nodei++)
            for (int nodej = nodei+1; nodej < tot; nodej++)
            {
                int oldi = old_id[nodei], oldj = old_id[nodej];

                double total_similarity = 0.0;
                // 对 nodei 和 nodej 这一对节点（子树），枚举它们所包含的单词，两两计算相似性再求和
                // 归一化在构图的函数 BuildGraph 和 BuildGraph2 中，此处先不归一化
                // 注：优化：如果保存了之前计算的所有相似度的中间结果，可以枚举 oldi 和 oldj 的子树，
                //  然后子树的相似性总和通过查表获得，而不必枚举所有单词（叶节点）从头算
                for (int id1 = subtree_leaf_list_head[oldi]; id1 != -1; id1 = e2[id1].next)
                    for (int id2 = subtree_leaf_list_head[oldj]; id2 != -1; id2 = e2[id2].next)
                    {
                        total_similarity += sim_mat_value(e2[id1].r, e2[id2].r);
                    }
                if (total_similarity > 1e-8)
                {
                    // printf("total_similarity = %f\n", total_similarity);
                    Edge2.insert(make_pair(make_pair(nodei, nodej), total_similarity));
                }
                if (++dbg_processed_pairs % (total_pairs/5+1) == 0)
                    printf("processed: %.2f\n", (double)dbg_processed_pairs / total_pairs);
            }
        printf("Calculating similarity ends!\n");

        // 将构建的相似性图写入文件中以便检查
        char graph_file_name[20];
        sprintf(graph_file_name, "graph%02d.txt", dep);
        printf("depth = %d, graph_file_name = %s\n", dep, graph_file_name);
        if (DebugGraph)
            fgraph = fopen(graph_file_name, "w");
        PerfectMatching *pm = BuildGraph2(tot, Edge2, subtree_size, old_id, node_num, edge_num);
        if (DebugGraph)
            fclose(fgraph);
        clock_t solve_time1 = clock();
        pm->Solve();                            // 调用模板进行最大权匹配
        clock_t solve_time2 = clock();
        printf("Total matching time: %f\n", (double)(solve_time2 - solve_time1) / CLOCKS_PER_SEC);
        int numMatchs = 0;
        // 找到上一层的一个内部节点 x
        for (x = Floor_head[dep-1].next; x != -1 && x < vocab_size; x = Floor[x].next);
        for (i = 0; i < tot; ++i)                   // 把匹配的点对两个两个地连向上一层的同一个父亲
        {
            j = pm->GetMatch(i);
            if (i < j && j < tot)
            {
                numMatchs += 1;

                l = old_id[i];
                r = old_id[j];
                lson[x] = l; rson[x] = r;

                if (DebugGraph && (dep == maxDep)) //输出匹配上的节点 及 new_id
                {
                    fprintf(adjusted_match_file, "%s %s (%d %d)\n", \
                      vocab[l].word, vocab[r].word, i, j);
                }

                //之前修改的变量都是自己新开辟的，这里才更改了word2vec中本来就有的变量 binary 和 parent_node
                binary[l] = 0; binary[r] = 1;
                parent_node[l] = parent_node[r] = x;
                if (x == -1) printf("Error at line %d: %d %d %d\n", __LINE__, dep, l, r);
                // 找到上一层的另一个内部节点 x
                for (x = Floor[x].next; x != -1 && x < vocab_size; x = Floor[x].next);
            }
        }
        printf("number of matches: %d\n", numMatchs);
        flag = 0; // flag = 0 和 1 分别对应于 左子树 和 右子树
        //注：优化：这个循环可以跟上面的循环合并
        for (i = 0; i < tot; ++i)       // 未匹配的点随意连向上一层
        {
            j = pm->GetMatch(i);
            if (j >= tot)
            {
                y = old_id[i];

                if (DebugGraph && (dep == maxDep)) //输出未匹配上的节点 及 new_id
                {
                    fprintf(adjusted_match_file, "%s (%d)\n", vocab[y].word, i);
                }

                if (x == -1) printf("Error at line %d: %d %d\n", __LINE__, dep, y);
                if (flag) {
                    rson[x] = y;
                    binary[y] = 1;
                    parent_node[y] = x;
                    // 找到上一层的另一个内部节点 x
                    for (x = Floor[x].next; x != -1 && x < vocab_size; x = Floor[x].next);
                } else {
                    lson[x] = y;
                    binary[y] = 0;
                    parent_node[y] = x;
                }
                flag ^= 1;
            }
        }
        delete pm;
        printf("\n");

        if (DebugGraph && (dep == maxDep)) // 记得要关闭文件
        {
            fclose(old_match_file);
            fclose(adjusted_match_file);
            free(old_match);
        }
    }
    printf("Finished adjusting Huffman tree\n");
    free(Floor);
    free(lson);
    free(rson);
    free(new_id);
    free(old_id);
    free(belong);
    free(Dep);
    free(subtree_leaf_list_head);
    free(e2);

    clock_t Adjust_huffman_time2 = clock();
    printf("Finish Adjusting Binary Tree Using Similarity Matrix! time = %.4f\n", \
    (double)(Adjust_huffman_time2-Adjust_huffman_time1)/CLOCKS_PER_SEC);
}


//把Huffman树写入到文件中
void DumpHuffman(char* file_name) {
	FILE* fh = fopen(file_name, "w");
	if (fh)
	{
		int a, b;
		for (a=0; a<vocab_size; a++)
		{
			for (b=0; b<vocab[a].codelen; b++)
				fprintf(fh, "%c", vocab[a].code[b]+'0');
			fprintf(fh, " %s\n", vocab[a].word);
		}
		fclose(fh);
	}
    else
    {
        printf("Error when opening file %s\n", file_name);
        exit(5);
    }
}

// Create binary Huffman tree using the word counts
// Frequent words will have short uniqe binary codes
void CreateBinaryTree() {
    long long a, b, i, min1i, min2i, pos1, pos2, point[MAX_CODE_LENGTH];
    char code[MAX_CODE_LENGTH];
    long long *count = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));
    binary = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));
    parent_node = (long long *)calloc(vocab_size * 2 + 1, sizeof(long long));

    // Huffman树的双数组实现
    // count[0..vocab_size-1]为语料库里出现过的单词（也即叶节点）的词频，词频从高到低降序排列。
    // count[vocab_size..vocab_size*2-2]为内部节点的词频，词频从低到高升序排列。先初始化成无穷。
    for (a = 0; a < vocab_size; a++) count[a] = vocab[a].cn;
    for (a = vocab_size; a < vocab_size * 2; a++) count[a] = 1e15;

    // pos1和pos2分别指向 叶节点 和 内部节点 中词频最低的项，即两个队列的队头
    pos1 = vocab_size - 1;
    pos2 = vocab_size;

    // Following algorithm constructs the Huffman tree by adding one node at a time

    for (a = 0; a < vocab_size - 1; a++) {
        // min1i 和 min2i分别为词频 最低 和 次低 的项（包括叶节点和内部节点）。
        if (pos1 >= 0) {
            if (count[pos1] < count[pos2]) {
                min1i = pos1;
                pos1--;
            } else {
                min1i = pos2;
                pos2++;
            }
        } else {
            min1i = pos2;
            pos2++;
        }
        if (pos1 >= 0) {
            if (count[pos1] < count[pos2]) {
                min2i = pos1;
                pos1--;
            } else {
                min2i = pos2;
                pos2++;
            }
        } else {
            min2i = pos2;
            pos2++;
        }
        // 构造内部节点，id设为 vocab_size + a，同时设置父节点指针
        // 并将右子节点的 Huffman编码置为1（左子节点的编码已初始化成0，不用修改）
        count[vocab_size + a] = count[min1i] + count[min2i];
        parent_node[min1i] = vocab_size + a;
        parent_node[min2i] = vocab_size + a;
        binary[min2i] = 1;
    }

    /// Modified by Ruan Chong
    /// Assign binary code to each vocabulary word
    for (a = 0; a < vocab_size; a++) { //遍历每个单词（叶节点）
        b = a;
        i = 0;
        while (1) { // 回溯至根节点（vocab_size * 2 - 2）.
          // code[0]是叶节点的编码, code[end]是其除根节点外最高层祖先的编码
          // point[0]是叶节点的id，point[end]是其除根节点外最高层祖先的id
            code[i] = binary[b];
            point[i] = b;
            i++;
            b = parent_node[b];
            if (b == vocab_size * 2 - 2) break; // 根节点
        }
        vocab[a].codelen = i;
        // 重编码: 给内部节点的编码都减去 vocab_size，也即从0开始
        // vacab[a].point[0]：根节点的id
        vocab[a].point[0] = vocab_size - 2;
        // 上面回溯以后编码是逆序的（从叶到根），point[]也是从叶节点到根
        // 现在把它们倒转过来，变成正的，i.e.: 从根到叶
        // 注意：  1, point[]比code[]有效长度长1,因为根节点没有Huffman编码
        //        2, point[end]是负的，因为它是用单词本身的id 减去 vocab_size得到的，
        //            只不过因为这一项后面的程序中没有使用这个数据，所以不会造成影响。
        for (b = 0; b < i; b++) {
            vocab[a].code[i - b - 1] = code[b];
            vocab[a].point[i - b] = point[b] - vocab_size;
        }
    }

    DumpHuffman("before-adjusting.huff");
    // Modification ends.

    if (ontology_file)
    {
        AdjustBinaryTree();	// 调整Huffman
    }
    else if (sim_file)
    {
        AdjustBinaryTreeUsingSimMatrix();
    }
    // Now assign binary code to each vocabulary word
    for (a = 0; a < vocab_size; a++) {
        b = a;
        i = 0;
        while (1) {
            code[i] = binary[b];
            point[i] = b;
            i++;
            b = parent_node[b];
            if (b == vocab_size * 2 - 2) break;
        }
        vocab[a].codelen = i;
        vocab[a].point[0] = vocab_size - 2;
        for (b = 0; b < i; b++) {
            vocab[a].code[i - b - 1] = code[b];
            vocab[a].point[i - b] = point[b] - vocab_size;
        }
    }
    DumpHuffman("after-adjusting.huff");

    free(count);
    free(binary);
    free(parent_node);
}

void LearnVocabFromTrainFile() {
    char word[MAX_STRING];
    FILE *fin;
    long long a, i;
    for (a = 0; a < vocab_hash_size; a++) vocab_hash[a] = -1;
    fin = fopen(train_file, "rb");
    if (fin == NULL) {
        printf("ERROR: training data file not found!\n");
        exit(1);
    }
    vocab_size = 0;
    AddWordToVocab((char *)"</s>");
    while (1) {
        ReadWord(word, fin);
        if (feof(fin)) break;
        train_words++;
        if ((debug_mode > 1) && (train_words % 100000 == 0)) {
            printf("%lldK%c", train_words / 1000, 13);
            fflush(stdout);
        }
        i = SearchVocab(word);
        if (i == -1) {
            a = AddWordToVocab(word);
            vocab[a].cn = 1;
        } else vocab[i].cn++;
        if (vocab_size > vocab_hash_size * 0.7) ReduceVocab();
    }
    SortVocab();
    if (debug_mode > 0) {
        printf("Vocab size: %lld\n", vocab_size);
        printf("Words in train file: %lld\n", train_words);
    }
    file_size = ftell(fin);
    fclose(fin);
}

void SaveVocab() {
    long long i;
    FILE *fo = fopen(save_vocab_file, "wb");
    for (i = 0; i < vocab_size; i++)
    {
        fprintf(fo, "%s %lld ", vocab[i].word, vocab[i].cn);
        int j;
        for (j=0; j<vocab[i].codelen; j++)
          fprintf(fo, "%c", vocab[i].code[j]+'0');
        fprintf(fo, "\n");
    }
    fclose(fo);
    printf("Vocabulary saved to %s\n", save_vocab_file);
}

void ReadVocab() {
    long long a, i = 0;
    char c;
    char word[MAX_STRING];
    FILE *fin = fopen(read_vocab_file, "rb");
    if (fin == NULL) {
        printf("Vocabulary file not found\n");
        exit(1);
    }
    for (a = 0; a < vocab_hash_size; a++) vocab_hash[a] = -1;
    vocab_size = 0;
    while (1) {
        ReadWord(word, fin);
        if (feof(fin)) break;
        a = AddWordToVocab(word);
        settle_down_compiler_int = fscanf(fin, "%lld%c", &vocab[a].cn, &c);
        i++;
    }
    SortVocab();
    if (debug_mode > 0) {
        printf("Vocab size: %lld\n", vocab_size);
        printf("Words in train file: %lld\n", train_words);
    }
    fin = fopen(train_file, "rb");
    if (fin == NULL) {
        printf("ERROR: training data file not found!\n");
        exit(1);
    }
    fseek(fin, 0, SEEK_END);
    file_size = ftell(fin);
    fclose(fin);
}


/// Modified by Ruan Chong
void ReadWordList() {
    FILE* fin = fopen(wordlist_file, "rb");
    if (fin == NULL)
    {
        printf("Word List not found\n");
        exit(1);
    }
    int id;
    char word[MAX_STRING], buff[MAX_STRING];
    settle_down_compiler_pointer = fgets(buff, MAX_STRING, fin);
    sscanf(buff, "%*s %lld", &word_list_vocab_size);
    printf("# of words = %lld\n", word_list_vocab_size);
    settle_down_compiler_pointer = fgets(buff, MAX_STRING, fin);

    index_in_word_list = (int *)malloc(vocab_hash_size * sizeof(int));
    if (index_in_word_list == NULL)
    {
        printf("Memory allocation for index_in_word_list failed!\n");
        exit(2);
    }
    memset(index_in_word_list, -1, vocab_hash_size * sizeof(int));

    printf("Wordlist file content:\n");
    int cnt = 0;
    while (fgets(buff, MAX_STRING, fin) != NULL)
    {
        sscanf(buff, "%d %s", &id, word);
        // printf("id = %d, word = %s\n", id, word);
        int ind = SearchVocab(word);
        if (ind != -1)
        {
            index_in_word_list[ind] = id;
            if (++cnt<5)
                printf("word = %s, id in word2vec = %d, id in word list = %d\n", word, ind, id);
        }
        // if (cnt > 1000)
        //     break;
    }
    fclose(fin);
    printf("finish reading word list!\n");
}


/// Modified by Ruan Chong
void ReadSimilarityMatrix() {
    FILE* fin = fopen(sim_file, "rb");
    clock_t t1 = clock();
    if (fin == NULL)
    {
        printf("Similarity matrix not found\n");
        exit(1);
    }
    int a, b;
    double sim;
    //double* sim_mat;
    //a = posix_memalign((void **)&sim_mat, 128, (long long)vocab_size * layer1_size * sizeof(real));
    sim_mat = (double*)calloc((word_list_vocab_size+1) * top_k, sizeof(double));
    if (sim_mat == NULL)
    {
        printf("Memory allocation for similarity matrix content failed!\n");
        exit(2);
    }

    ind_mat = (int*)calloc((word_list_vocab_size+1) * top_k, sizeof(int));
    if (ind_mat == NULL)
    {
        printf("Memory allocation for similarity matrix index failed!\n");
        exit(2);
    }

    printf("Similarity matrix file content:\n");
    int cnt = 0;
    while (fscanf(fin, "%d %d %lf", &a, &b, &sim) == 3)
    {
        sim_mat_insert(a, b, sim);
        sim_mat_insert(b, a, sim);
        if (cnt < 10)
            printf("a = %d, b = %d, sim(a,b) = %lf\n", a, b, sim);
        if (++cnt % 100000000 == 0)
            printf("processed %d lines...\n", cnt);
    }
    fclose(fin);
    clock_t t2 = clock();
    printf("finish reading similarity matrix! time eclipsed = %.3f\n", (double)(t2-t1)/CLOCKS_PER_SEC);
}


void ReadOntology()	{	// 读入ontology，存放在block中，每一组在同一行
    int tmp[MAX_BLOCK];
    char word[MAX_STRING];
    FILE *fin = fopen(ontology_file, "r");
    if (fin == NULL)
    {
        printf("Ontology file not found\n");
        exit(1);
    }
    int size, real_size, i, j, p;
    block = (int**)calloc(block_max_size, sizeof(int*));
    block_size = 0;
    while (1)
    {
        settle_down_compiler_int = fscanf(fin, "%d", &size);
        if (size == -1) break;
        real_size = 0;
        for (j = 0; j < size; ++j)
        {
            settle_down_compiler_int = fscanf(fin, "%s", word);
            p = SearchVocab(word);
            if (p != -1) tmp[++real_size] = p;
        }
        if (real_size > 1)
        {
            if (block_size + 2 >= block_max_size)
            {
                block_max_size += 1000;
                block = (int**)realloc(block, block_max_size * sizeof(int*));
            }
            block[block_size] = (int*)calloc(real_size+2, sizeof(int));
            for (i = 1; i <= real_size; ++i)
                block[block_size][i] = tmp[i];
            block[block_size++][0] = real_size;
        }
    }
}

void InitNet() {
    long long a, b;
    unsigned long long next_random = 1;
    a = posix_memalign((void **)&syn0, 128, (long long)vocab_size * layer1_size * sizeof(real));
    if (syn0 == NULL) {printf("Memory allocation failed\n"); exit(1);}
    if (hs) {
        a = posix_memalign((void **)&syn1, 128, (long long)vocab_size * layer1_size * sizeof(real));
        if (syn1 == NULL) {printf("Memory allocation failed\n"); exit(1);}
        for (a = 0; a < vocab_size; a++) for (b = 0; b < layer1_size; b++)
            syn1[a * layer1_size + b] = 0;
    }
    if (negative>0) {
        a = posix_memalign((void **)&syn1neg, 128, (long long)vocab_size * layer1_size * sizeof(real));
        if (syn1neg == NULL) {printf("Memory allocation failed\n"); exit(1);}
        for (a = 0; a < vocab_size; a++) for (b = 0; b < layer1_size; b++)
            syn1neg[a * layer1_size + b] = 0;
    }
    for (a = 0; a < vocab_size; a++) for (b = 0; b < layer1_size; b++) {
        next_random = next_random * (unsigned long long)25214903917 + 11;
        syn0[a * layer1_size + b] = (((next_random & 0xFFFF) / (real)65536) - 0.5) / layer1_size;
    }
    CreateBinaryTree();
}


// Dump decision vector syn1[] in Hierachical Softmax Layer
void DumpSyn1(char* file_name = "hierachical-softmax.para") {
    int d, k, word, pos, cnt;
    printf("\nBegin dumping hierachical softmax parameters!\n");
    hs_para = (hs_parameter *)calloc(vocab_size, sizeof(hs_parameter));
    if (hs_para == NULL)
    {
        printf("Memory Allocation Failed when dumping Hierachical Softmax parameters\n");
        exit(1);
    }

    // Establish map from indices of syn1[] to Huffman suffix-free code
    // For 0 <= k < vocab_size, parameter syn1[k * layer1_size .. (k+1) * layer1_size]
    // corresponds to suffix-free code hs_para[k].code[]
    bool* visited = (bool*)calloc(vocab_size, sizeof(bool));
    for (word = 0, cnt = 0; word < vocab_size; word++)
        for (d = 0; d < vocab[word].codelen; d++)
        {
            pos = vocab[word].point[d];
            if (visited[pos])       continue;
            cnt += 1;
            visited[pos] = true;
            hs_para[pos].codelen = d+1;
            hs_para[pos].code = (char *)calloc(MAX_CODE_LENGTH, sizeof(char));
            if (hs_para[pos].code == NULL)
            {
                printf("Memory Allocation Failed when copying suffix-free codes\n");
                exit(2);
            }
            if (d>0)
            {
                for (k=0; k<d; k++)
                    hs_para[pos].code[k] = vocab[word].code[k] + '0';
                hs_para[pos].code[k] = 0;
            }
            else // in case of generating an empty code string
            {
                strcpy(hs_para[pos].code, "ROOT");
            }
        }
    printf("%d parameters found in total, which should be %d theoretically\n", cnt, vocab_size-1);

    // Write parameters to file.
    // Format:
    //      First line: <number of parameter vectors> <dimension of parameter vectors>
    //      Following lines: <prefix-free code> <parameter vector>, seperated by white space.
    FILE* fh = fopen(file_name, "w");
    fprintf(fh, "%d %d\n", cnt, layer1_size);
    if (fh != NULL)
    {
        for (pos = 0; pos < vocab_size; pos++)
            if (hs_para[pos].code != NULL)
            {
                fprintf(fh, "%s ", hs_para[pos].code);
                for (k = 0; k < layer1_size; k++)
                    fprintf(fh, "%lf ", syn1[pos * layer1_size + k]);
                fprintf(fh, "\n");
                // printf("pos = %d, codelen = %d, len(code) = %d\n", pos, hs_para[pos].codelen, strlen(hs_para[pos].code));
            }
        fclose(fh);
    }
    else
    {
        printf("Failed to open file %s\n", file_name);
        exit(3);
    }

    // Free memories allocated before
    for (pos = 0; pos < vocab_size; pos++)
        if (hs_para[pos].code != NULL)
            free(hs_para[pos].code);
    free(hs_para);
    printf("Finish dumping hierachical softmax parameters!\n");
}

void *TrainModelThread(void *id) {
    long long a, b, d, cw, word, last_word, sentence_length = 0, sentence_position = 0;
    long long word_count = 0, last_word_count = 0, sen[MAX_SENTENCE_LENGTH + 1];
    long long l1, l2, c, target, label;
    unsigned long long next_random = (long long)id;
    real f, g;
    clock_t now;
    real *neu1 = (real *)calloc(layer1_size, sizeof(real));
    real *neu1e = (real *)calloc(layer1_size, sizeof(real));
    FILE *fi = fopen(train_file, "rb");
    fseek(fi, file_size / (long long)num_threads * (long long)id, SEEK_SET);
    while (1) {
        if (word_count - last_word_count > 10000) {
            word_count_actual += word_count - last_word_count;
            last_word_count = word_count;
            if ((debug_mode > 1)) {
                now=clock();
                printf("%cAlpha: %f  Progress: %.2f%%  Words/thread/sec: %.2fk  ", 13, alpha,
                        word_count_actual / (real)(iter * train_words + 1) * 100,
                        word_count_actual / ((real)(now - start + 1) / (real)CLOCKS_PER_SEC * 1000));
                fflush(stdout);
            }
            alpha = starting_alpha * (1 - word_count_actual / (real)(iter * train_words + 1));
            if (alpha < starting_alpha * 0.0001) alpha = starting_alpha * 0.0001;
        }
        if (sentence_length == 0) {
            while (1) {
                word = ReadWordIndex(fi);
                if (feof(fi)) break;
                if (word == -1) continue;
                word_count++;
                if (word == 0) break;
                // The subsampling randomly discards frequent words while keeping the ranking same
                if (sample > 0) {
                    real ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;
                    next_random = next_random * (unsigned long long)25214903917 + 11;
                    if (ran < (next_random & 0xFFFF) / (real)65536) continue;
                }
                sen[sentence_length] = word;
                sentence_length++;
                if (sentence_length >= MAX_SENTENCE_LENGTH) break;
            }
            sentence_position = 0;
        }
        if (feof(fi) || (word_count > train_words / num_threads)) {
            word_count_actual += word_count - last_word_count;
            break;
        }
        word = sen[sentence_position];
        if (word == -1) continue;
        for (c = 0; c < layer1_size; c++) neu1[c] = 0;
        for (c = 0; c < layer1_size; c++) neu1e[c] = 0;
        next_random = next_random * (unsigned long long)25214903917 + 11;
        b = next_random % window;
        // true_window = window - b
        // current sample: sent[sentence_position - true_window : sentence_position - 1]
        // and sent[sentence_position + 1 : sentence_position + true_window]
        if (cbow) {  //train the cbow architecture
            // in -> hidden
            cw = 0;
            for (a = b; a < window * 2 + 1 - b; a++) if (a != window) {
                c = sentence_position - window + a;
                if (c < 0) continue;
                if (c >= sentence_length) continue;
                last_word = sen[c];
                if (last_word == -1) continue;
                for (c = 0; c < layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];
                cw++;
            }
            if (cw) {
                for (c = 0; c < layer1_size; c++) neu1[c] /= cw;
                if (hs) for (d = 0; d < vocab[word].codelen; d++) {
                    f = 0;
                    l2 = vocab[word].point[d] * layer1_size;
                    // Propagate hidden -> output
                    for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1[c + l2];
                    if (f <= -MAX_EXP) continue;
                    else if (f >= MAX_EXP) continue;
                    else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
                    // 'g' is the gradient multiplied by the learning rate
                    g = (1 - vocab[word].code[d] - f) * alpha;
                    // Propagate errors output -> hidden
                    for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];
                    // Learn weights hidden -> output
                    for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * neu1[c];
                }
                // NEGATIVE SAMPLING
                if (negative > 0) for (d = 0; d < negative + 1; d++) {
                    if (d == 0) {
                        target = word;
                        label = 1;
                    } else {
                        next_random = next_random * (unsigned long long)25214903917 + 11;
                        target = table[(next_random >> 16) % table_size];
                        if (target == 0) target = next_random % (vocab_size - 1) + 1;
                        if (target == word) continue;
                        label = 0;
                    }
                    l2 = target * layer1_size;
                    f = 0;
                    for (c = 0; c < layer1_size; c++) f += neu1[c] * syn1neg[c + l2];
                    if (f > MAX_EXP) g = (label - 1) * alpha;
                    else if (f < -MAX_EXP) g = (label - 0) * alpha;
                    else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;
                    for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];
                    for (c = 0; c < layer1_size; c++) syn1neg[c + l2] += g * neu1[c];
                }
                // hidden -> in
                for (a = b; a < window * 2 + 1 - b; a++) if (a != window) {
                    c = sentence_position - window + a;
                    if (c < 0) continue;
                    if (c >= sentence_length) continue;
                    last_word = sen[c];
                    if (last_word == -1) continue;
                    for (c = 0; c < layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c];
                }
            }
        } else {  //train skip-gram
            for (a = b; a < window * 2 + 1 - b; a++) if (a != window) {
                c = sentence_position - window + a;
                if (c < 0) continue;
                if (c >= sentence_length) continue;
                last_word = sen[c];
                if (last_word == -1) continue;
                l1 = last_word * layer1_size;
                for (c = 0; c < layer1_size; c++) neu1e[c] = 0;
                // HIERARCHICAL SOFTMAX
                if (hs) for (d = 0; d < vocab[word].codelen; d++) {
                    f = 0;
                    l2 = vocab[word].point[d] * layer1_size;
                    // Propagate hidden -> output
                    for (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1[c + l2];
                    if (f <= -MAX_EXP) continue;
                    else if (f >= MAX_EXP) continue;
                    else f = expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))];
                    // 'g' is the gradient multiplied by the learning rate
                    g = (1 - vocab[word].code[d] - f) * alpha;
                    // Propagate errors output -> hidden
                    for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1[c + l2];
                    // Learn weights hidden -> output
                    for (c = 0; c < layer1_size; c++) syn1[c + l2] += g * syn0[c + l1];
                }
                // NEGATIVE SAMPLING
                if (negative > 0) for (d = 0; d < negative + 1; d++) {
                    if (d == 0) {
                        target = word;
                        label = 1;
                    } else {
                        next_random = next_random * (unsigned long long)25214903917 + 11;
                        target = table[(next_random >> 16) % table_size];
                        if (target == 0) target = next_random % (vocab_size - 1) + 1;
                        if (target == word) continue;
                        label = 0;
                    }
                    l2 = target * layer1_size;
                    f = 0;
                    for (c = 0; c < layer1_size; c++) f += syn0[c + l1] * syn1neg[c + l2];
                    if (f > MAX_EXP) g = (label - 1) * alpha;
                    else if (f < -MAX_EXP) g = (label - 0) * alpha;
                    else g = (label - expTable[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha;
                    for (c = 0; c < layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];
                    for (c = 0; c < layer1_size; c++) syn1neg[c + l2] += g * syn0[c + l1];
                }
                // Learn weights input -> hidden
                for (c = 0; c < layer1_size; c++) syn0[c + l1] += neu1e[c];
            }
        }
        sentence_position++;
        if (sentence_position >= sentence_length) {
            sentence_length = 0;
            continue;
        }
    }
    fclose(fi);
    free(neu1);
    free(neu1e);
    pthread_exit(NULL);
}

void saveVectors(FILE *fo)
{
    fprintf(fo, "%lld %lld\n", vocab_size, layer1_size);
    long a, b;
    for (a = 0; a < vocab_size; a++) {
        fprintf(fo, "%s ", vocab[a].word);
        if (binary_sign) for (b = 0; b < layer1_size; b++) fwrite(&syn0[a * layer1_size + b], sizeof(real), 1, fo);
        else for (b = 0; b < layer1_size; b++) fprintf(fo, "%lf ", syn0[a * layer1_size + b]);
        fprintf(fo, "\n");
    }
}

void TrainModel() {
    long a, b, c, d;
    FILE *fo;
    pthread_t *pt = (pthread_t *)malloc(num_threads * sizeof(pthread_t));
    printf("Starting training using file %s\n", train_file);
    starting_alpha = alpha;
    if (read_vocab_file[0] != 0) ReadVocab(); else LearnVocabFromTrainFile();
    if (save_vocab_file[0] != 0) SaveVocab();
    if (output_file[0] == 0) return;
    if (ontology_file) ReadOntology();		// 读入ontology

    // Read Similarity matrix and word list file.
    // When doing this, vocab learned from text has been load into memory.
    if (sim_file && wordlist_file)
    {
        SaveVocabDebug("learned-vocab-before-read-word-list.txt");
        ReadWordList();
        SaveVocabDebug("learned-vocab-mid-read-similarity.txt");
        ReadSimilarityMatrix();
        SaveVocabDebug("learned-vocab-after-read-similarity.txt");
    }
    InitNet();
    if (negative > 0) InitUnigramTable();
    start = clock();
    for(int current_iter = iter; current_iter > 0; current_iter--) {
        for (a = 0; a < num_threads; a++) pthread_create(&pt[a], NULL, TrainModelThread, (void *)a);
        for (a = 0; a < num_threads; a++) pthread_join(pt[a], NULL);
        if(testing) {
            int finished = (int)iter - current_iter + 1;
            if((finished - 1) % testing == 0){
                printf("\nFinished %d-th iteration, perfrom testing:\n", (int)iter - current_iter + 1);
                fo = fopen(output_file, "wb");
                saveVectors(fo);
                fclose(fo);

                char cmd[1024];
                sprintf(cmd, "cd evaluation.jl/; julia -p 4 evaluation.jl ../%s %d; cd ..", output_file, (int)iter - current_iter + 1);
                system(cmd);
            }
        }
    }
    fo = fopen(output_file, "wb");
    if (classes == 0) {
        // Save the word vectors
        fprintf(fo, "%lld %lld\n", vocab_size, layer1_size);
        for (a = 0; a < vocab_size; a++) {
            fprintf(fo, "%s ", vocab[a].word);
            if (binary_sign) for (b = 0; b < layer1_size; b++) fwrite(&syn0[a * layer1_size + b], sizeof(real), 1, fo);
            else for (b = 0; b < layer1_size; b++) fprintf(fo, "%lf ", syn0[a * layer1_size + b]);
            fprintf(fo, "\n");
        }
    } else {
        // Run K-means on the word vectors
        int clcn = classes, iter = 10, closeid;
        int *centcn = (int *)malloc(classes * sizeof(int));
        int *cl = (int *)calloc(vocab_size, sizeof(int));
        real closev, x;
        real *cent = (real *)calloc(classes * layer1_size, sizeof(real));
        for (a = 0; a < vocab_size; a++) cl[a] = a % clcn;
        for (a = 0; a < iter; a++) {
            for (b = 0; b < clcn * layer1_size; b++) cent[b] = 0;
            for (b = 0; b < clcn; b++) centcn[b] = 1;
            for (c = 0; c < vocab_size; c++) {
                for (d = 0; d < layer1_size; d++) cent[layer1_size * cl[c] + d] += syn0[c * layer1_size + d];
                centcn[cl[c]]++;
            }
            for (b = 0; b < clcn; b++) {
                closev = 0;
                for (c = 0; c < layer1_size; c++) {
                    cent[layer1_size * b + c] /= centcn[b];
                    closev += cent[layer1_size * b + c] * cent[layer1_size * b + c];
                }
                closev = sqrt(closev);
                for (c = 0; c < layer1_size; c++) cent[layer1_size * b + c] /= closev;
            }
            for (c = 0; c < vocab_size; c++) {
                closev = -10;
                closeid = 0;
                for (d = 0; d < clcn; d++) {
                    x = 0;
                    for (b = 0; b < layer1_size; b++) x += cent[layer1_size * d + b] * syn0[c * layer1_size + b];
                    if (x > closev) {
                        closev = x;
                        closeid = d;
                    }
                }
                cl[c] = closeid;
            }
        }
        // Save the K-means classes
        for (a = 0; a < vocab_size; a++) fprintf(fo, "%s %d\n", vocab[a].word, cl[a]);
        free(centcn);
        free(cent);
        free(cl);
    }
    fclose(fo);
}

int ArgPos(char *str, int argc, char **argv) {
    int a;
    for (a = 1; a < argc; a++) if (!strcmp(str, argv[a])) {
        if (a == argc - 1) {
            printf("Argument missing for %s\n", str);
            exit(1);
        }
        return a;
    }
    return -1;
}


int main(int argc, char **argv) {
    int i;
    if (argc == 1) {
        printf("WORD VECTOR estimation toolkit v 0.1c\n\n");
        printf("Options:\n");
        printf("Parameters for training:\n");
        printf("\t-train <file>\n");
        printf("\t\tUse text data from <file> to train the model\n");
        printf("\t-output <file>\n");
        printf("\t\tUse <file> to save the resulting word vectors / word clusters\n");
        printf("\t-size <int>\n");
        printf("\t\tSet size of word vectors; default is 100\n");
        printf("\t-window <int>\n");
        printf("\t\tSet max skip length between words; default is 5\n");
        printf("\t-sample <float>\n");
        printf("\t\tSet threshold for occurrence of words. Those that appear with higher frequency in the training data\n");
        printf("\t\twill be randomly down-sampled; default is 1e-3, useful range is (0, 1e-5)\n");
        printf("\t-hs <int>\n");
        printf("\t\tUse Hierarchical Softmax; default is 0 (not used)\n");
        printf("\t-negative <int>\n");
        printf("\t\tNumber of negative examples; default is 5, common values are 3 - 10 (0 = not used)\n");
        printf("\t-threads <int>\n");
        printf("\t\tUse <int> threads (default 12)\n");
        printf("\t-iter <int>\n");
        printf("\t\tRun more training iterations (default 5)\n");
        printf("\t-min-count <int>\n");
        printf("\t\tThis will discard words that appear less than <int> times; default is 5\n");
        printf("\t-alpha <float>\n");
        printf("\t\tSet the starting learning rate; default is 0.025 for skip-gram and 0.05 for CBOW\n");
        printf("\t-classes <int>\n");
        printf("\t\tOutput word classes rather than word vectors; default number of classes is 0 (vectors are written)\n");
        printf("\t-debug <int>\n");
        printf("\t\tSet the debug mode (default = 2 = more info during training)\n");
        printf("\t-binary <int>\n");
        printf("\t\tSave the resulting vectors in binary moded; default is 0 (off)\n");
        printf("\t-save-vocab <file>\n");
        printf("\t\tThe vocabulary will be saved to <file>\n");
        printf("\t-read-vocab <file>\n");
        printf("\t\tThe vocabulary will be read from <file>, not constructed from the training data\n");
        printf("\t-cbow <int>\n");
        printf("\t\tUse the continuous bag of words model; default is 1 (use 0 for skip-gram model)\n");
        printf("\nExamples:\n");
        printf("./my_word2vec -train data10000.txt -output vec.txt -size 200 -window 5 -sample 1e-4 -negative 0 -hs 1 -binary 0 -cbow 1 -iter 3 -ontology ../data/ontology1.txt.utf\n\n");
        return 0;
    }
    output_file[0] = 0;
    save_vocab_file[0] = 0;
    read_vocab_file[0] = 0;
    if ((i = ArgPos((char *)"-size", argc, argv)) > 0) layer1_size = atoi(argv[i + 1]);
    if ((i = ArgPos((char *)"-train", argc, argv)) > 0) strcpy(train_file, argv[i + 1]);
    if ((i = ArgPos((char *)"-save-vocab", argc, argv)) > 0) strcpy(save_vocab_file, argv[i + 1]);
    if ((i = ArgPos((char *)"-read-vocab", argc, argv)) > 0) strcpy(read_vocab_file, argv[i + 1]);
    if ((i = ArgPos((char *)"-debug", argc, argv)) > 0) debug_mode = atoi(argv[i + 1]);
    if ((i = ArgPos((char *)"-binary", argc, argv)) > 0) binary_sign = atoi(argv[i + 1]);
    if ((i = ArgPos((char *)"-cbow", argc, argv)) > 0) cbow = atoi(argv[i + 1]);
    if (cbow) alpha = 0.05;
    if ((i = ArgPos((char *)"-alpha", argc, argv)) > 0) alpha = atof(argv[i + 1]);
    if ((i = ArgPos((char *)"-output", argc, argv)) > 0) strcpy(output_file, argv[i + 1]);
    if ((i = ArgPos((char *)"-window", argc, argv)) > 0) window = atoi(argv[i + 1]);
    if ((i = ArgPos((char *)"-sample", argc, argv)) > 0) sample = atof(argv[i + 1]);
    if ((i = ArgPos((char *)"-hs", argc, argv)) > 0) hs = atoi(argv[i + 1]);
    if ((i = ArgPos((char *)"-negative", argc, argv)) > 0) negative = atoi(argv[i + 1]);
    if ((i = ArgPos((char *)"-threads", argc, argv)) > 0) num_threads = atoi(argv[i + 1]);
    if ((i = ArgPos((char *)"-iter", argc, argv)) > 0) iter = atoi(argv[i + 1]);
    if ((i = ArgPos((char *)"-min-count", argc, argv)) > 0) min_count = atoi(argv[i + 1]);
    if ((i = ArgPos((char *)"-classes", argc, argv)) > 0) classes = atoi(argv[i + 1]);
    if ((i = ArgPos((char *)"-ontology", argc, argv)) > 0) ontology_file = argv[i + 1];
    if ((i = ArgPos((char *)"-wordlist", argc, argv)) > 0) wordlist_file = argv[i + 1];
    if ((i = ArgPos((char *)"-sim", argc, argv)) > 0) sim_file = argv[i + 1];
    if ((i = ArgPos((char *)"-testing", argc, argv)) > 0) testing = atoi(argv[i + 1]);
    vocab = (struct vocab_word *)calloc(vocab_max_size, sizeof(struct vocab_word));
    vocab_hash = (int *)calloc(vocab_hash_size, sizeof(int));
    expTable = (real *)malloc((EXP_TABLE_SIZE + 1) * sizeof(real));
    for (i = 0; i < EXP_TABLE_SIZE; i++) {
        expTable[i] = exp((i / (real)EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // Precompute the exp() table
        expTable[i] = expTable[i] / (expTable[i] + 1);                   // Precompute f(x) = x / (x + 1)
    }
    TrainModel();
    if (hs)
    	DumpSyn1("hierachical-softmax.para");
    return 0;
}
